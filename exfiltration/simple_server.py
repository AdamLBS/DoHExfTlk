#!/usr/bin/env python3
"""
DoH Exfiltration Server - Version Simplifi√©e

Simple serveur pour capturer et reconstruire les donn√©es exfiltr√©es via DoH.
Se concentre uniquement sur la capture et reconstruction, sans d√©tection complexe.
"""

import os
import sys
import time
import logging
import threading
from pathlib import Path
from traffic_interceptor import DoHTrafficInterceptor

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SimpleExfiltrationServer:
    """Serveur simple pour capturer l'exfiltration DoH"""
    
    def __init__(self, output_dir="/app/captured", interface=None):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.interface = interface or os.environ.get('INTERFACE', 'eth0')
        
        # Sessions de reconstruction
        self.sessions = {}
        self.session_lock = threading.Lock()
        
        logger.info(f"Serveur d'exfiltration d√©marr√©")
        logger.info(f"Interface: {self.interface}")
        logger.info(f"R√©pertoire de sortie: {self.output_dir}")
    
    def handle_dns_query(self, query_data):
        """Traite une requ√™te DNS captur√©e"""
        try:
            domain = query_data.get('domain', '')
            query_name = query_data.get('query_name', '')
            
            # D√©tecter les requ√™tes d'exfiltration (domaines suspects)
            if any(keyword in domain.lower() for keyword in ['exfill', 'data', 'leak']):
                logger.info(f"üéØ Requ√™te d'exfiltration d√©tect√©e: {query_name}")
                
                # Extraire les donn√©es potentielles
                self._extract_exfiltration_data(query_data)
                
        except Exception as e:
            logger.error(f"Erreur lors du traitement de la requ√™te: {e}")
    
    def _extract_exfiltration_data(self, query_data):
        """Extrait et stocke les donn√©es d'exfiltration"""
        try:
            query_name = query_data.get('query_name', '')
            timestamp = query_data.get('timestamp', time.time())
            
            # Analyser le format: session_id-index-total-chunk.random.domain
            parts = query_name.split('.')
            if len(parts) >= 2:
                data_segment = parts[0]  # Premier segment avant le premier point
                
                # Parser le format: session_id-index-total-chunk
                import re
                pattern = re.compile(r"(\d+)-(\d{4})-(\d{4})-([a-zA-Z0-9_\-]+=*)")
                match = pattern.match(data_segment)
                
                if match:
                    session_id = match.group(1)  # Utiliser le vrai session ID, pas l'IP
                    index = int(match.group(2))
                    total = int(match.group(3))
                    chunk = match.group(4)
                    
                    with self.session_lock:
                        if session_id not in self.sessions:
                            self.sessions[session_id] = {
                                'start_time': timestamp,
                                'total_chunks': total,
                                'chunks': {},  # Utiliser un dict pour l'indexation
                                'queries': []
                            }
                        
                        # Stocker le chunk avec son index
                        self.sessions[session_id]['chunks'][index] = chunk
                        self.sessions[session_id]['queries'].append(query_data)
                    
                    logger.info(f"üì¶ Segment de donn√©es captur√©: {session_id}-{index:04d}-{total:04d}-{chunk[:20]}...")
                    
                    # V√©rifier si on a tous les chunks
                    session = self.sessions[session_id]
                    if len(session['chunks']) >= session['total_chunks']:
                        logger.info(f"üîß Reconstruction compl√®te pour session {session_id} ({session['total_chunks']} chunks)")
                        self._try_reconstruct_session(session_id)
                else:
                    logger.warning(f"‚ö†Ô∏è Format de chunk non reconnu: {data_segment}")
                
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction: {e}")
            import traceback
            traceback.print_exc()
    
    def _try_reconstruct_session(self, session_id):
        """Tente de reconstruire les donn√©es d'une session"""
        try:
            session = self.sessions[session_id]
            chunks = session['chunks']
            total_chunks = session['total_chunks']
            
            # V√©rifier que tous les chunks sont pr√©sents
            if len(chunks) == total_chunks:
                logger.info(f"üîß Reconstruction compl√®te pour session {session_id} ({total_chunks} chunks)")
                
                # Ordonner les chunks par index
                ordered_chunks = []
                for i in range(total_chunks):
                    if i in chunks:
                        ordered_chunks.append(chunks[i])
                    else:
                        logger.error(f"‚ùå Chunk manquant: {i}")
                        return
                
                # Reconstruire les donn√©es
                reconstructed_data = self._decode_chunks(ordered_chunks)
                
                if reconstructed_data:
                    # Sauvegarder les donn√©es reconstruites
                    output_file = self.output_dir / f"exfiltrated_{session_id}_{int(time.time())}.bin"
                    with open(output_file, 'wb') as f:
                        f.write(reconstructed_data)
                    
                    logger.info(f"‚úÖ Donn√©es reconstruites sauv√©es: {output_file}")
                    logger.info(f"üìä Taille: {len(reconstructed_data)} bytes")
                    
                    # Afficher un aper√ßu si c'est du texte
                    try:
                        preview = reconstructed_data[:200].decode('utf-8', errors='ignore')
                        logger.info(f"üìÑ Aper√ßu: {preview}...")
                    except:
                        logger.info("üìÑ Donn√©es binaires d√©tect√©es")
                    
                    # Nettoyer la session
                    del self.sessions[session_id]
            else:
                logger.info(f"‚è≥ Session {session_id}: {len(chunks)}/{total_chunks} chunks re√ßus")
                    
        except Exception as e:
            logger.error(f"Erreur lors de la reconstruction: {e}")
            import traceback
            traceback.print_exc()
    
    def _decode_chunks(self, ordered_chunks):
        """D√©code les chunks ordonn√©s en donn√©es originales"""
        import base64
        
        # Concat√©ner tous les chunks dans l'ordre
        combined_data = ''.join(ordered_chunks)
        logger.info(f"üî¢ Donn√©es combin√©es: {len(combined_data)} caract√®res")
        
        # Essayer le d√©codage base64 URL-safe en premier (utilis√© par le client)
        try:
            # Ajouter le padding n√©cessaire pour base64
            padding_needed = 4 - (len(combined_data) % 4)
            if padding_needed != 4:
                combined_data += '=' * padding_needed
            
            decoded = base64.urlsafe_b64decode(combined_data)
            logger.info(f"‚úÖ D√©codage r√©ussi avec base64 URL-safe")
            return decoded
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è √âchec base64 URL-safe: {e}")
        
        # Essayer base64 standard
        try:
            decoded = base64.b64decode(combined_data + '=' * (4 - len(combined_data) % 4))
            logger.info(f"‚úÖ D√©codage r√©ussi avec base64 standard")
            return decoded
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è √âchec base64 standard: {e}")
        
        # Essayer base32
        try:
            padding_needed = 8 - (len(combined_data) % 8)
            if padding_needed != 8:
                combined_data += '=' * padding_needed
            decoded = base64.b32decode(combined_data)
            logger.info(f"‚úÖ D√©codage r√©ussi avec base32")
            return decoded
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è √âchec base32: {e}")
        
        # Essayer hex
        try:
            decoded = bytes.fromhex(combined_data)
            logger.info(f"‚úÖ D√©codage r√©ussi avec hex")
            return decoded
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è √âchec hex: {e}")
        
        logger.error("‚ùå Aucun d√©codage r√©ussi")
        return None
    
    def start(self):
        """D√©marre le serveur de capture"""
        logger.info("üöÄ D√©marrage de la capture...")
        
        try:
            # Cr√©er l'intercepteur de trafic
            interceptor = DoHTrafficInterceptor(
                interface=self.interface,
                output_dir=str(self.output_dir)
            )
            
            # Configurer le callback pour traiter les requ√™tes
            interceptor.dns_callback = self.handle_dns_query
            
            # D√©marrer la capture
            interceptor.start_capture()
            
        except KeyboardInterrupt:
            logger.info("üõë Arr√™t du serveur...")
        except Exception as e:
            logger.error(f"‚ùå Erreur fatale: {e}")
            sys.exit(1)

def main():
    """Point d'entr√©e principal"""
    
    # Configuration depuis les variables d'environnement
    output_dir = os.environ.get('OUTPUT_DIR', '/app/captured')
    interface = os.environ.get('INTERFACE')
    
    # Cr√©er et d√©marrer le serveur
    server = SimpleExfiltrationServer(
        output_dir=output_dir,
        interface=interface
    )
    
    server.start()

if __name__ == "__main__":
    main()
